{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic experiments for the paper: \"On Lazy Training in Differentiable Programming\"\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot, ProgressMeter\n",
    "using Random, LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent for $2$-layers neural net (fixed training and test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient descent to train a 2-layers ReLU neural net for the square loss and with a scaling:\n",
    "F(w) = MSE(scaling*f(w))/scaling^2\n",
    "\"\"\"\n",
    "function GDfor2NN(X_train, X_test, Y_train, Y_test, W_init, scaling, stepsize, niter) \n",
    "    (n,d) = size(X_train)\n",
    "    m     = size(W_init, 1)\n",
    "    W     = copy(W_init)   \n",
    "    Ws   = zeros(m, d+1, niter)# store optimization path\n",
    "    loss_train = zeros(niter)\n",
    "    loss_test = zeros(niter)\n",
    "    for iter = 1:niter\n",
    "        Ws[:,:,iter] = W\n",
    "        # output of the neural net\n",
    "        temp    =  max.( W[:,1:end-1] * X_train', 0.0) # output hidden layer (size m × n)\n",
    "        output  = scaling * sum( W[:,end] .* temp , dims=1) # output network (size 1 × n)\n",
    "        # compute gradient\n",
    "        gradR   = (output .- Y_train)'/n  # size n\n",
    "        grad_w1 = (W[:,end] .* float.(temp .> 0) * ( X_train .* gradR )) # (size m × d) \n",
    "        grad_w2 = temp * gradR # size m\n",
    "        grad = cat(grad_w1, grad_w2, dims=2) # size (m × d+1)   \n",
    "        # store train loss\n",
    "        loss_train[iter] = (1/2)*sum( ( output - Y_train).^2 )/n\n",
    "        # store test loss  \n",
    "        output = scaling .* sum( W[:,end] .* max.( W[:,1:end-1] * X_test', 0.0) , dims=1)\n",
    "        loss_test[iter] = (1/2)*sum( ( output - Y_test).^2 )/length(Y_test)\n",
    "        # gradient descent\n",
    "        W = W - (stepsize/scaling) * grad\n",
    "    end\n",
    "    Ws, loss_train, loss_test\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent for $2$-layers neural net (directly minimizes the population loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SGD to train a 2-layers ReLU neural net of size m for the square loss and with a scaling:\n",
    "F(w) = MSE(scaling*f(w))/scaling^2\n",
    "teacher parameters are w0, θ0\n",
    "\"\"\"\n",
    "function populationSGDfor2NN(m, w0, θ0, stepsize, batchsize, scale, niter)  \n",
    "    m0,d = size(θ0)\n",
    "    θ = scale * randn(m,d) # start gradient flow with normalized data at fixed distance\n",
    "    w = scale * randn(m,1)\n",
    "    w[1:div(m,2)] = abs.(w[1:div(m,2)])\n",
    "    w[(div(m,2)+1):end] = - abs.(w[1:div(m,2)])\n",
    "    θ[(div(m,2)+1):end,:] = θ[1:div(m,2),:] # symmetrization\n",
    "\n",
    "    θs = zeros(m,d,niter) # storing neurons\n",
    "    ws = zeros(m,niter)   # storing output homogenizers\n",
    "    val= zeros(niter,1)   # storing loss\n",
    "    \n",
    "    σ(x) = max(x,0) # ReLU activation\n",
    "    σ′(x) = float(x>0)\n",
    "    sigmaderinc(s) = float(s>0)\n",
    "\n",
    "    # gradient flow\n",
    "    for iter = 1:niter\n",
    "        θs[:,:,iter] = θ\n",
    "        ws[:,iter] = w\n",
    "        # random data points\n",
    "        X = randn(batchsize,d)\n",
    "        X = X ./ sqrt.(sum(X.^2, dims=2))\n",
    "        Y0 = sum( w0 .* σ.(θ0*X'), dims=1)/batchsize  #ground truth output\n",
    "    \n",
    "        # prediction and gradient computation\n",
    "        temp = σ.( θ * X')    \n",
    "        Y = sum( w .* temp, dims=1)/m\n",
    "        val[iter] = (1/2)*sum( ( Y - Y0).^2 )/batchsize;\n",
    "        gradR =  ( Y - Y0 )'/batchsize; # column of size batchsize\n",
    "        gradw = temp * gradR\n",
    "        gradθ = ((w.*sigmaderinc.(temp)) * ( X .* gradR ))#./(1+w)*2000\n",
    "        \n",
    "\n",
    "        θ = θ - stepsize * gradθ*d\n",
    "        w = w - stepsize * gradw\n",
    "    end\n",
    "    ws,θs,val\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of the GD dynamics in 2-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1);\n",
    "include(\"illustration.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of the scale of initialization on generalization (GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1)\n",
    "include(\"test_vs_scale.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of m on generalization, with two scalings (GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(2)\n",
    "include(\"test_vs_m.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of scaling with pure SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(2)\n",
    "include(\"populationSGD.jl\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
